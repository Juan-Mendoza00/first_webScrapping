{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projecto Integrador\n",
    "\n",
    "Primer acercamiento a trabajar con el concepto de web Scrapping. El proyecto fué propuesto en clase por el maestro Gonzalo del Rio, durante el primer módulo del Bootcamp de Henry.\n",
    "\n",
    "El proyecto consiste en hacer WebScrapping al sitio web https://cuspide.com/ para extraer la información de la sección de \"100 libros más vendidos de la semana\" en un formato permita hacer el procesamiento con Pandas y crear finalmente un archivo csv de salida.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para empezar:\n",
    "\n",
    "#### 1) Preparar el ambiente. \n",
    "\n",
    "Se importan las librerías de procesamiento de los datos con las que se va a trabajar.\n",
    "\n",
    "- Se usa Pandas para el procesamiento y visualización de los datos. **Se requiere crear un DataFrame.**\n",
    "- Se usa el módulo `requests` para enviar requerimentos HTTP al sitio que nos interesa. Como respuesta queremos obtener el HTML crudo de la página.\n",
    "- Se usa `Beautiful Soup` para leer el HTML en una estructura que sea útil para extraer la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable containing the website's url.\n",
    "url = 'https://cuspide.com/100-mas-vendidos/' \n",
    "\n",
    "# Making the HTTP request using request.get() method.\n",
    "# A response object is returned.\n",
    "response = requests.get(url) \n",
    "\n",
    "# Getting raw html from response.content attribute.\n",
    "html_content = response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) BeautifulSoup \n",
    "\n",
    "Beautiful soup interpreta el contenido html crudo y lo organiza en estructura jerárquica, parecido a como lo hace el navegador. Con la estructura de etiquetas donde podemos analizar esa jerarquía ya se puede empezar a estraer la información. \n",
    "\n",
    "Esa es la magia de Beautiful soup :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the content with bs4\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de inspeccionar la página, se observó que los titulos están contenidos en una etiqueta `<h3>` con el atributo de clase \"product-title\".\n",
    "\n",
    "**Nota**: El método `find_all()` retorna un objeto tipo ResultSet que contiene todas las etiquetas que cumplan con los parámetros que se pasen por la función. Este ResultSet se puede iterar para trabajar con cada etiqueta específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA MUJER QUE SOY\n",
      "ARTIFICIAL\n",
      "ESTE DOLOR NO ES MIO\n",
      "LA CASA NEVILLE . LA FORMIDABLE SEÑORITA  MANON\n",
      "OXIDO\n"
     ]
    }
   ],
   "source": [
    "# Finding title's tags for each book\n",
    "title_tags = soup.find_all(\"h3\", class_='product-title')\n",
    "\n",
    "for tag in title_tags[:5]: # Loop over each tag\n",
    "   print(tag.get_text(strip=True)) # Printing text within every tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo las etiquetas, podemos guardarlas en una lista (usando list comprehension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La Mujer Que Soy',\n",
       " 'Artificial',\n",
       " 'Este Dolor No Es Mio',\n",
       " 'La Casa Neville . La Formidable Señorita  Manon',\n",
       " 'Oxido']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a list with titles\n",
    "titulos = [tag.get_text(strip=True).title() for tag in title_tags]\n",
    "titulos[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "... Por inspección tambíen puede verse que los url de cada libro están contenidos en una etiqueta \"hija\" dentro de las mismas etiquetas de los títulos.\n",
    "\n",
    "**Nota**: El objeto etiqueta nos permite acceder a las etiquetas que tiene por dentro (estructura jerárquica de arbol). Con el atritubo .attrs podemos ver un diccionario con los atributos de la etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://cuspide.com/producto/la-mujer-que-soy/',\n",
       " 'https://cuspide.com/producto/artificial-2/',\n",
       " 'https://cuspide.com/producto/este-dolor-no-es-mio/',\n",
       " 'https://cuspide.com/producto/la-casa-neville-la-formidable-senorita-manon/',\n",
       " 'https://cuspide.com/producto/oxido/']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a list with urls for every book detail\n",
    "urls = [tag.a.attrs['href'] for tag in title_tags]\n",
    "urls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para los precios se usó un método de distinto (para ilustrar que hay diferentes formas de hacerlo).\n",
    "\n",
    "En lugar de find_all, puede usarse el método selector de css que de igual manera es muy cómodo para seleccionar las etiquetas por clases, estilos y id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$8.999,00\n",
      "$9.999,00\n",
      "$11.500,00\n",
      "$13.700,00\n",
      "$14.999,00\n"
     ]
    }
   ],
   "source": [
    "# Selecting tags by css classes\n",
    "price_tags = soup.css.select('.product-price')\n",
    "\n",
    "for tag in price_tags[:5]:\n",
    "    print(tag.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8999.0, 9999.0, 11500.0, 13700.0, 14999.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making list for prices. Getting only numeric part.\n",
    "price_column = [tag.get_text(strip=True)[1:] for tag in price_tags]\n",
    "\n",
    "# Formatting and casting to float.\n",
    "price_column = [float(price.replace(\".\",\"\").replace(\",\",\".\")) for price in price_column]\n",
    "price_column[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que el precio en dólares se encuentra en cada url específico para cada libro, he creado funciones scrapper para esta tarea.\n",
    "Las funciones están definidas dentro del módulo `functions.py`. Allí puede verse el script que se ha creado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import get_usdprice_and_date\n",
    "\n",
    "# Getting usd price for each book.\n",
    "usd_prices = []\n",
    "dates = [] \n",
    "for url in urls:\n",
    "    usd_price, date = get_usdprice_and_date(url)\n",
    "    usd_prices.append(usd_price)\n",
    "    dates.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['24,62', '27,36', '31,46', '37,48', '41,04'],\n",
       " ['31/10/2023', '28/09/2023', '16/03/2018', '29/09/2023', '06/10/2023'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usd_prices[:5], dates[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([24.62, 27.36, 31.46, 37.48, 41.04],\n",
       " ['2023-10-31', '2023-09-28', '2018-03-16', '2023-09-29', '2023-10-06'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formatting and casting usd prices\n",
    "usd_prices = [float(price.replace(\",\",\".\")) for price in usd_prices]\n",
    "usd_prices[:5]\n",
    "\n",
    "# Formatting dates to SQL format\n",
    "from functions import format_date\n",
    "dates = [format_date(date, old_format='%d/%m/%Y', new_format='%Y-%m-%d') for date in dates]\n",
    "\n",
    "usd_prices[:5], dates[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obteniendo el precio del Dolar Blue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.cronista.com/MercadosOnline/dolar.html')\n",
    "dom_content = r.text\n",
    "html_soup = BeautifulSoup(dom_content, 'html.parser')\n",
    "\n",
    "dolar_blue = html_soup.find('span', string='Dólar Blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "925.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Span tag containing value is the next one. \"The sibling\"\n",
    "dolar_blue.next_sibling.text[1:]\n",
    "# Formatting and casting.\n",
    "dolarBlue_precio = float(dolar_blue.next_sibling.text[1:].replace(\",\",\".\"))\n",
    "dolarBlue_precio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.73, 10.81, 12.43, 14.81, 16.22]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making list with dolar blue conversion.\n",
    "blue_column = [round(precio / dolarBlue_precio, 2) for precio in price_column]\n",
    "blue_column[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Creación del archivo de salida:\n",
    "\n",
    "- The following step is to create a .csv output file.\n",
    "- Using csv module for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('La Mujer Que Soy',\n",
       "  '2023-10-31',\n",
       "  'https://cuspide.com/producto/la-mujer-que-soy/',\n",
       "  8999.0,\n",
       "  24.62,\n",
       "  9.73),\n",
       " ('Artificial',\n",
       "  '2023-09-28',\n",
       "  'https://cuspide.com/producto/artificial-2/',\n",
       "  9999.0,\n",
       "  27.36,\n",
       "  10.81),\n",
       " ('Este Dolor No Es Mio',\n",
       "  '2018-03-16',\n",
       "  'https://cuspide.com/producto/este-dolor-no-es-mio/',\n",
       "  11500.0,\n",
       "  31.46,\n",
       "  12.43)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = []\n",
    "for i in range(len(title_tags)):\n",
    "    table.append(\n",
    "        (titulos[i], dates[i], urls[i], price_column[i], usd_prices[i], blue_column[i])\n",
    "    )\n",
    "\n",
    "# Now each tuple is the equivalent to a row\n",
    "table[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the csv file\n",
    "with open('./libros_semana.csv', 'w') as file:\n",
    "    w = csv.writer(file, delimiter=',') # Writer object\n",
    "    w.writerow(('titulo', 'fecha_publicación', 'url', 'precio_pesos', 'precio_usd', 'precio_dolar_blue'))\n",
    "    w.writerows(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "2da Parte del proyecto - Módulo 3\n",
    "\n",
    "## Conexión con MySQL: Creando una base de datos.\n",
    "\n",
    "En esta sección se crea una base de datos MySQL en lugar de un archivo csv. También se implementa manejo de excepciones para crear una tabla de errores cuando los haya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating connection\n",
    "connection = pymysql.Connection(\n",
    "    host='localhost',\n",
    "    user='juancml',\n",
    "    passwd='',\n",
    "    database='libros'\n",
    ")\n",
    "\n",
    "# creating mysql cursor\n",
    "cursor = connection.cursor()\n",
    "# Query to insert values on mysql table.\n",
    "query = \"\"\" INSEERT INTO libros_semana \n",
    "            VALUES (%s,%s,%s,%s,%s,%s, CURRDATE()) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace la insercion de los datos con el método del cursor `executemany()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.executemany(query, table)\n",
    "# Commit changes and close database connection\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
